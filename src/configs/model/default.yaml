defaults:
  - loss: cross_entropy
  - encoder: many_token_v2

_target_: model.Model

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  weight_decay: 0.0
  betas: [0.9, 0.95]

scheduler:
  _target_: torch.optim.lr_scheduler.SequentialLR
  _partial_: true
  schedulers:
    - _target_: torch.optim.lr_scheduler.LinearLR
      _partial_: true
      start_factor: 1e-5
      end_factor: 1.0
      # missing total_iters, which would be the warmup

    - _target_: torch.optim.lr_scheduler.CosineAnnealingLR
      _partial_: True
      T_max: ??? # total_iters - warmup_iters
      eta_min: 0.0

# compile model for faster training with pytorch 2.0
compile: False
