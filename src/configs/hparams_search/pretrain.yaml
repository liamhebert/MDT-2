# @package _global_

# example hyperparameter optimization of some experiment with Optuna:
# python train.py -m hparams_search=mnist_optuna experiment=example

defaults:
  - default

# choose metric which will be optimized by Optuna
# make sure this is the correct name of some metric logged in lightning module!
optimized_metric: "val/best_loss_epoch"

# here we define Optuna hyperparameter search
# it optimizes for value returned from function with @hydra.main decorator
# docs: https://hydra.cc/docs/next/plugins/optuna_sweeper
hydra:
  sweeper:
    # number of parallel workers
    n_jobs: 8

    # 'minimize' or 'maximize' the objective
    direction: minimize

    # total number of runs that will be executed
    n_trials: 20

    # define hyperparameter search space
    params:
      model.optimizer.lr: choice(0.001) # choose between 0.001 to 0.01
      model.encoder.graph_stack_factory.num_layers: choice(2, 3) # choose between 2, 3
      model.encoder.graph_stack_factory.graph_tfmr_factory.use_rope: choice(False, True) # choose between True, False
      model.encoder.graph_stack_factory.graph_tfmr_factory.differential_attention: choice(False, True) # choose between True, False
