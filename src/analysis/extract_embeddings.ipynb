{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import hydra\n",
    "import rootutils\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, open_dict\n",
    "\n",
    "os.environ[\"IS_PROD\"] = \"1\"\n",
    "rootutils.setup_root(\"../\", indicator=\".project-root\", pythonpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import DataModule\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/u2/l2hebert/MDT-2/src/logs/train/multiruns/2025-03-18_23-26-47/7/checkpoints/last.ckpt\"\n",
    "\n",
    "\n",
    "def make_hydra_config(overrides: list[str]):\n",
    "    with initialize(config_path=\"../configs/\"):\n",
    "        cfg = compose(\n",
    "            config_name=\"train.yaml\",\n",
    "            overrides=overrides,\n",
    "            return_hydra_config=True,\n",
    "        )\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg = make_hydra_config(\n",
    "    [\n",
    "        \"experiment=pretrain_siglip\",\n",
    "        \"model.encoder.graph_stack_factory.num_layers=3\",\n",
    "        \"model.encoder.graph_stack_factory.graph_tfmr_factory.use_rope=False\",\n",
    "        \"model.encoder.graph_stack_factory.graph_tfmr_factory.differential_attention=True\",\n",
    "        \"model.encoder.graph_stack_factory.graph_tfmr_factory.n_heads=16\",\n",
    "        \"model.encoder.graph_stack_factory.graph_tfmr_factory.n_kv_heads=16\",\n",
    "        \"model.encoder.graph_stack_factory.graph_tfmr_factory.head_dim=128\",\n",
    "        \"paths.root_dir='../'\",\n",
    "        \"dataset.test_batch_size=8\",\n",
    "        \"dataset.train_batch_size=1\",\n",
    "        \"dataset.group_size=1\",\n",
    "    ]\n",
    ")\n",
    "HydraConfig().set_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder: Model = hydra.utils.instantiate(cfg.model.encoder)\n",
    "loss = hydra.utils.instantiate(cfg.model.loss)\n",
    "\n",
    "model = Model.load_from_checkpoint(ckpt_path, encoder=encoder, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hydra.utils.instantiate(cfg.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._val_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = dataset.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_device(batch, device):\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            batch[k] = v.to(device, non_blocking=True)\n",
    "        if isinstance(v, dict):\n",
    "            batch[k] = send_to_device(v, device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        x, y = batch[\"x\"], batch[\"y\"]\n",
    "        x = send_to_device(x, \"cuda:0\")\n",
    "        y = send_to_device(y, \"cuda:0\")\n",
    "        _, graph_embeddings = model.forward(x)\n",
    "        embeddings.append(graph_embeddings.cpu().numpy())\n",
    "        labels.append(y[\"ys\"].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "output_embeddings = np.concat(embeddings, axis=0)\n",
    "output_labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "output_embeddings_mini = output_embeddings[output_labels != 1]\n",
    "output_labels_mini = output_labels[output_labels != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def sigmoid(x: np.ndarray, y: np.ndarray):\n",
    "    def apply_sig(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def apply_norm(z):\n",
    "        return z / np.linalg.norm(z, ord=2)\n",
    "\n",
    "    x = apply_norm(x)\n",
    "    y = apply_norm(y)\n",
    "    return apply_sig(x.dot(y.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "mapper = umap.UMAP(metric=sigmoid, n_neighbors=100).fit(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings = (\n",
    "    output_embeddings\n",
    "    / np.linalg.norm(output_embeddings, ord=2, axis=1)[:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = np.matmul(output_embeddings, output_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(sim, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_flat = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vals = {\"labels\": labels_flat, \"preds\": labels_flat[sim.argmax(axis=1)]}\n",
    "df_vals = pd.DataFrame(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vals[\"match\"] = df_vals[\"labels\"] == df_vals[\"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.concatenate(labels, axis=0), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "\n",
    "umap.plot.points(mapper, labels=output_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
