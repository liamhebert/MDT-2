{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "rootutils.setup_root(\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\n",
    "    \"Partisan A\": {\n",
    "        \"a\": [\n",
    "            \"democrats\",\n",
    "            \"OpenChristian\",\n",
    "            \"GamerGhazi\",\n",
    "            \"excatholic\",\n",
    "            \"EnoughLibertarianSpam\",\n",
    "            \"AskAnAmerican\",\n",
    "            \"lastweektonight\",\n",
    "        ],\n",
    "        \"b\": [\n",
    "            \"Conservative\",\n",
    "            \"progun\",\n",
    "            \"TrueChristian\",\n",
    "            \"Catholicism\",\n",
    "            \"AskTrumpSupporters\",\n",
    "            \"CGPGrey\",\n",
    "        ],\n",
    "    },\n",
    "    \"Partisan B\": {\n",
    "        \"a\": [\n",
    "            \"hillaryclinton\",\n",
    "            \"SandersForPresident\",\n",
    "            \"askhillarysupporters\",\n",
    "            \"BlueMidterm2018\",\n",
    "            \"badwomensanatomy\",\n",
    "            \"PoliticalVideo\",\n",
    "            \"liberalgunowners\",\n",
    "            \"GrassrootsSelect\",\n",
    "            \"GunsAreCool\",\n",
    "        ],\n",
    "        \"b\": [\n",
    "            \"The_Donald\",\n",
    "            \"KotakuInAction\",\n",
    "            \"HillaryForPrison\",\n",
    "            \"AskThe_Donald\",\n",
    "            \"PoliticalHumor\",\n",
    "            \"ChoosingBeggars\",\n",
    "            \"uncensorednews\",\n",
    "            \"Firearms\",\n",
    "            \"DNCleaks\",\n",
    "            \"dgu\",\n",
    "        ],\n",
    "    },\n",
    "    \"Affluence\": {\n",
    "        \"a\": [\n",
    "            \"vagabond\",\n",
    "            \"hitchhiking\",\n",
    "            \"DumpsterDiving\",\n",
    "            \"almosthomeless\",\n",
    "            \"AskACountry\",\n",
    "            \"KitchenConfidential\",\n",
    "            \"Nightshift\",\n",
    "            \"alaska\",\n",
    "            \"fuckolly\",\n",
    "            \"FolkPunk\",\n",
    "        ],\n",
    "        \"b\": [\n",
    "            \"backpacking\",\n",
    "            \"hiking\",\n",
    "            \"Frugal\",\n",
    "            \"personalfinance\",\n",
    "            \"travel\",\n",
    "            \"Cooking\",\n",
    "            \"fitbit\",\n",
    "            \"CampingandHiking\",\n",
    "            \"gameofthrones\",\n",
    "            \"IndieFolk\",\n",
    "        ],\n",
    "    },\n",
    "    \"Gender\": {\n",
    "        \"a\": [\n",
    "            \"AskMen\",\n",
    "            \"TrollYChromosome\",\n",
    "            \"AskMenOver30\",\n",
    "            \"OneY\",\n",
    "            \"TallMeetTall\",\n",
    "            \"daddit\",\n",
    "            \"ROTC\",\n",
    "            \"FierceFlow\",\n",
    "            \"malelivingspace\",\n",
    "            \"predaddit\",\n",
    "        ],\n",
    "        \"b\": [\n",
    "            \"AskWomen\",\n",
    "            \"CraftyTrolls\",\n",
    "            \"AskWomenOver30\",\n",
    "            \"women\",\n",
    "            \"bigboobproblems\",\n",
    "            \"Mommit\",\n",
    "            \"USMilitarySO\",\n",
    "            \"HaircareScience\",\n",
    "            \"InteriorDesign\",\n",
    "            \"BabyBumps\",\n",
    "        ],\n",
    "    },\n",
    "    \"Age\": {\n",
    "        \"a\": [\n",
    "            \"teenagers\",\n",
    "            \"youngatheists\",\n",
    "            \"teenrelationships\",\n",
    "            \"AskMen\",\n",
    "            \"saplings\",\n",
    "            \"hsxc\",\n",
    "            \"trackandfield\",\n",
    "            \"bapccanada\",\n",
    "            \"RedHotChiliPeppers\",\n",
    "        ],\n",
    "        \"b\": [\n",
    "            \"RedditForGrownups\",\n",
    "            \"TrueAtheism\",\n",
    "            \"relationship_advice\",\n",
    "            \"AskMenOver30\",\n",
    "            \"eldertrees\",\n",
    "            \"running\",\n",
    "            \"trailrunning\",\n",
    "            \"MaleFashionMarket\",\n",
    "            \"canadacordcutters\",\n",
    "            \"pearljam\",\n",
    "        ],\n",
    "    },\n",
    "    # \"Edgy\": {\n",
    "    #     \"a\": [\n",
    "    #         \"memes\",\n",
    "    #         \"watchpeoplesurvive\",\n",
    "    #         \"MissingPersons\",\n",
    "    #         \"twinpeaks\",\n",
    "    #         \"pickuplines\",\n",
    "    #         \"texts\",\n",
    "    #         \"startrekgifs\",\n",
    "    #         \"subredditoftheday\",\n",
    "    #         \"peeling\",\n",
    "    #         \"rapbattles\",\n",
    "    #     ],\n",
    "    #     \"b\": [\n",
    "    #         \"ImGoingToHellForThis\",\n",
    "    #         \"watchpeopledie\",\n",
    "    #         \"MorbidReality\",\n",
    "    #         \"TrueDetective\",\n",
    "    #         \"MeanJokes\",\n",
    "    #         \"FiftyFifty\",\n",
    "    #         \"DaystromInstitute\",\n",
    "    #         \"SRSsucks\",\n",
    "    #         \"bestofworldstar\",\n",
    "    #     ],\n",
    "    # },\n",
    "}\n",
    "\n",
    "groups[\"Partisan A\"][\"a\"].extend(groups[\"Partisan B\"][\"a\"])\n",
    "groups[\"Partisan A\"][\"b\"].extend(groups[\"Partisan B\"][\"b\"])\n",
    "del groups[\"Partisan B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sizes_subreddit = {}\n",
    "sizes_global = {}\n",
    "size_numbers = {}\n",
    "\n",
    "\n",
    "def get_true_size(subreddit):\n",
    "    sizes = []\n",
    "    # print(os.getcwd())\n",
    "    # print(f\"../../data/processed_files/processed/{subreddit}/*\")\n",
    "    for file in tqdm(\n",
    "        glob(\n",
    "            f\"/scratch/rcohen/l2hebert/data/processed_files/processed/{subreddit}/*\"\n",
    "        )\n",
    "    ):\n",
    "        graph = torch.load(file, weights_only=False)\n",
    "        size = len(graph[\"in_degree\"])\n",
    "        sizes.append(size)\n",
    "    return sizes\n",
    "\n",
    "\n",
    "for file in glob(\"raw_files/*/*-data.json\"):\n",
    "    _, topic, subreddit = file.split(\"/\")\n",
    "    subreddit = subreddit.split(\"-\")[0]\n",
    "    count = len(open(file).readlines())\n",
    "    size_numbers[subreddit] = get_true_size(subreddit)\n",
    "    if \"Partisan\" in topic:\n",
    "        topic = \"Partisan A\"\n",
    "\n",
    "    if subreddit in groups[topic][\"a\"]:\n",
    "        topic = f\"{topic} A\"\n",
    "    elif subreddit in groups[topic][\"b\"]:\n",
    "        topic = f\"{topic} B\"\n",
    "    else:\n",
    "        print(f\"Subreddit {subreddit} in Topic {topic} not found in any group\")\n",
    "        continue\n",
    "\n",
    "    if \"Partisan A\" in topic:\n",
    "        topic = topic.replace(\"Partisan A\", \"Partisan\")\n",
    "    if \"Parisan B\" in topic:\n",
    "        topic = topic.replace(\"Partisan B\", \"Partisan\")\n",
    "    if topic not in sizes_subreddit:\n",
    "        sizes_subreddit[topic] = {}\n",
    "        sizes_global[topic] = 0\n",
    "\n",
    "    sizes_subreddit[topic][subreddit] = count\n",
    "    sizes_global[topic] += count\n",
    "\n",
    "\n",
    "pprint(sizes_global)\n",
    "pprint(sizes_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit, sizes in size_numbers.items():\n",
    "    num_to_remove = sum(1 if size < 5 else 0 for size in sizes)\n",
    "    total_size = len(sizes)\n",
    "    print(\n",
    "        f\"Subreddit {subreddit} ({total_size}) has {num_to_remove} graphs with less than 5 nodes\"\n",
    "    )\n",
    "    # Find what topic the subreddit belongs to\n",
    "    for topic, subreddits in sizes_subreddit.items():\n",
    "        if subreddit in subreddits:\n",
    "            sizes_subreddit[topic][subreddit] -= num_to_remove\n",
    "            sizes_global[topic] -= num_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = {}\n",
    "for subreddit, sizes in size_numbers.items():\n",
    "    indices_to_remove[subreddit] = []\n",
    "    for idx, size in enumerate(sizes):\n",
    "        if size < 5:\n",
    "            indices_to_remove[subreddit].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def calculate_balanced_ratios(\n",
    "    top_level_sizes, sub_dataset_sizes, holdout_ratio=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates balancing ratios (upsampling or downsampling) for sub-datasets to balance a hierarchical dataset,\n",
    "    accounting for in-balance between sub-datasets as well.\n",
    "\n",
    "    This function aims to achieve three levels of balancing:\n",
    "    1) Balance classes 'A' and 'B' within each top-level dataset towards a middle ground.\n",
    "    2) Balance the total size of each top-level dataset to each other towards a middle ground.\n",
    "    3) Balance the size of sub-datasets within each top-level dataset towards a middle ground.\n",
    "    4) Calculate balancing ratios for sub-datasets that contribute to 1, 2 & 3,\n",
    "       allowing for both upsampling and downsampling.\n",
    "\n",
    "    Args:\n",
    "        top_level_sizes (dict): Dictionary of top-level dataset sizes.\n",
    "                                 e.g., {'Affluence A': 9254, 'Affluence B': 36844, ...}\n",
    "        sub_dataset_sizes (dict): Nested dictionary of sub-dataset sizes.\n",
    "                                   e.g., {'Partisan A': {'liberalgunowners': 2389, 'excatholic': 260, ...},\n",
    "                                         'Partisan B': {'Conservative': 5000, 'Republican': 6000, ...}, ...}\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary with the same structure as sub_dataset_sizes,\n",
    "              but containing balancing ratios for each sub-dataset instead of sizes.\n",
    "              Ratios > 1 indicate upsampling, ratios < 1 indicate downsampling, ratio = 1 means no change.\n",
    "              e.g., {'Partisan A': {'liberalgunowners': 0.8, 'excatholic': 1.5, ...}, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    balancing_ratios = {}\n",
    "    balanced_top_level_sizes = {}\n",
    "\n",
    "    top_level_sizes = {\n",
    "        key: value * (1 - holdout_ratio)\n",
    "        for key, value in top_level_sizes.items()\n",
    "        if value > 0\n",
    "    }\n",
    "    sub_dataset_sizes = {\n",
    "        key: {k: v * (1 - holdout_ratio) for k, v in value.items() if v > 0}\n",
    "        for key, value in sub_dataset_sizes.items()\n",
    "    }\n",
    "\n",
    "    # 1. Balance A and B classes within each top-level dataset towards the average size\n",
    "    for topic_class, total_size in top_level_sizes.items():\n",
    "        topic, class_label = topic_class.split()\n",
    "        if topic not in balanced_top_level_sizes:\n",
    "            balanced_top_level_sizes[topic] = {\"A\": 0, \"B\": 0}\n",
    "        balanced_top_level_sizes[topic][class_label] = total_size\n",
    "\n",
    "    for topic_data in balanced_top_level_sizes.values():\n",
    "        avg_class_size = (\n",
    "            (topic_data[\"A\"] + topic_data[\"B\"]) / 2\n",
    "            if (topic_data[\"A\"] + topic_data[\"B\"]) > 0\n",
    "            else 0\n",
    "        )\n",
    "        topic_data[\"A_target_size\"] = avg_class_size\n",
    "        topic_data[\"B_target_size\"] = avg_class_size\n",
    "\n",
    "    # 2. Balance top-level datasets to each other towards the average total size\n",
    "    total_topic_sizes = []\n",
    "    for topic, topic_data in balanced_top_level_sizes.items():\n",
    "        total_topic_sizes.append(\n",
    "            topic_data[\"A_target_size\"] + topic_data[\"B_target_size\"]\n",
    "        )\n",
    "\n",
    "    avg_total_topic_size = (\n",
    "        sum(total_topic_sizes) / len(total_topic_sizes)\n",
    "        if total_topic_sizes\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    for topic_data in balanced_top_level_sizes.values():\n",
    "        topic_data[\"topic_target_size\"] = avg_total_topic_size\n",
    "\n",
    "    # 3. Calculate balancing ratios for sub-datasets, considering sub-dataset balance\n",
    "    for topic_class, sub_datasets in sub_dataset_sizes.items():\n",
    "        topic, class_label = topic_class.split()\n",
    "        balancing_ratios[topic_class] = {}\n",
    "        current_class_size = top_level_sizes[topic_class]\n",
    "        target_class_size = balanced_top_level_sizes[topic][\n",
    "            f\"{class_label}_target_size\"\n",
    "        ]\n",
    "        topic_target_size = balanced_top_level_sizes[topic][\"topic_target_size\"]\n",
    "        current_topic_size = (\n",
    "            balanced_top_level_sizes[topic][\"A_target_size\"]\n",
    "            + balanced_top_level_sizes[topic][\"B_target_size\"]\n",
    "        )  # sizes after A/B balancing\n",
    "\n",
    "        # Class level balancing ratio (for A/B balancing within topic)\n",
    "        class_balancing_factor = (\n",
    "            target_class_size / current_class_size\n",
    "            if current_class_size > 0\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "        # Topic level balancing ratio (for balancing topics to each other)\n",
    "        topic_balancing_factor = (\n",
    "            topic_target_size / current_topic_size\n",
    "            if current_topic_size > 0\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "        # Sub-dataset level balancing ratio (balance sub-datasets within the class)\n",
    "        values = [size for size in sub_datasets.values() if size > 0]\n",
    "        avg_sub_dataset_size = sum(values) / len(values) if sub_datasets else 0\n",
    "\n",
    "        for sub_dataset_name, sub_dataset_size in sub_datasets.items():\n",
    "            sub_dataset_size_adjusted = sub_dataset_size\n",
    "            if avg_sub_dataset_size == 0:\n",
    "                sub_dataset_balancing_factor = 1.0  # Avoid division by zero if no sub-datasets or avg size is zero\n",
    "            elif sub_dataset_size == 0:\n",
    "                sub_dataset_balancing_factor = (\n",
    "                    1.0  # if current sub dataset size is zero, no change needed\n",
    "                )\n",
    "            else:\n",
    "                # We clip the balancing factor to be between 0.5 and 5 to avoid\n",
    "                # extreme upsampling/downsampling\n",
    "                sub_dataset_balancing_factor = max(\n",
    "                    0.75,\n",
    "                    min(avg_sub_dataset_size / sub_dataset_size_adjusted, 5),\n",
    "                )\n",
    "\n",
    "            # Base ratio based on class balancing\n",
    "            base_ratio = class_balancing_factor\n",
    "\n",
    "            # Refine ratio to also contribute to topic balancing and sub-dataset balancing.\n",
    "\n",
    "            final_ratio = (\n",
    "                base_ratio\n",
    "                * topic_balancing_factor\n",
    "                * sub_dataset_balancing_factor\n",
    "            )\n",
    "            sub_dataset_ratio = final_ratio\n",
    "\n",
    "            balancing_ratios[topic_class][sub_dataset_name] = sub_dataset_ratio\n",
    "\n",
    "    return balancing_ratios\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "\n",
    "balancing_ratios_result = calculate_balanced_ratios(\n",
    "    deepcopy(sizes_global), deepcopy(sizes_subreddit)\n",
    ")\n",
    "pprint(balancing_ratios_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancing_ratios_result[\"Partisan A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_subreddit[\"Partisan A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def apply_balancing(sizes, ratios):\n",
    "    balanced_data = {}\n",
    "    for topic_class, sub_ratios in ratios.items():\n",
    "        print(f\"Balancing {topic_class}\")\n",
    "        print(sub_ratios.keys())\n",
    "        balanced_data[topic_class] = {}\n",
    "        for sub_dataset_name, ratio in sub_ratios.items():\n",
    "            print(\n",
    "                f\"Balancing {topic_class} {sub_dataset_name} with ratio {ratio}\"\n",
    "            )\n",
    "            num_to_remove = indices_to_remove[sub_dataset_name]\n",
    "            original_data = list(\n",
    "                range(sizes[topic_class][sub_dataset_name] + len(num_to_remove))\n",
    "            )  # Assume 'data' is your input data structure\n",
    "            original_data = [\n",
    "                idx for idx in original_data if idx not in num_to_remove\n",
    "            ]\n",
    "\n",
    "            train, test_validation = train_test_split(\n",
    "                original_data, test_size=0.2, random_state=42\n",
    "            )\n",
    "            if len(test_validation) < 20:\n",
    "                print(\n",
    "                    f\"Skipping validation split for {topic_class} {sub_dataset_name} due to insufficient data ({len(test_validation)} samples)\"\n",
    "                )\n",
    "                train += test_validation\n",
    "                test = []\n",
    "                validation = []\n",
    "            else:\n",
    "                test, validation = train_test_split(\n",
    "                    test_validation, test_size=0.5, random_state=42\n",
    "                )\n",
    "\n",
    "            original_size = len(train)\n",
    "            ratio = max(\n",
    "                0.5, min(ratio, 5)\n",
    "            )  # Clip ratio to be between 0.5 and 5\n",
    "            target_size = int(original_size * ratio)\n",
    "\n",
    "            random.seed(42)\n",
    "\n",
    "            if ratio > 1:  # Upsampling\n",
    "                # Implement upsampling logic (e.g., random oversampling)\n",
    "                new_train = random.choices(\n",
    "                    train, k=target_size\n",
    "                )  # Simple example\n",
    "            elif ratio < 1:  # Downsampling\n",
    "                # Implement downsampling logic (e.g., random undersampling)\n",
    "                new_train = random.sample(train, target_size)  # Simple example\n",
    "            else:  # ratio == 1, no change\n",
    "                new_train = train\n",
    "\n",
    "            splits = {\n",
    "                \"old_train_idx\": train,\n",
    "                \"train_idx\": new_train,\n",
    "                \"test_idx\": test,\n",
    "                \"valid_idx\": validation,\n",
    "            }\n",
    "            topic_folder = topic_class[:-2]\n",
    "            if topic_folder == \"Partisan\":\n",
    "                topic_folder = \"Partisan A\"\n",
    "            json.dump(\n",
    "                splits,\n",
    "                open(\n",
    "                    f\"raw_files/{topic_folder}/{sub_dataset_name}-split.json\",\n",
    "                    \"w\",\n",
    "                ),\n",
    "            )\n",
    "            balanced_data[topic_class][sub_dataset_name] = splits\n",
    "    return balanced_data\n",
    "\n",
    "\n",
    "# Assuming 'your_data' is your data in the same structure as sub_dataset_sizes\n",
    "balanced_dataset = apply_balancing(sizes_subreddit, balancing_ratios_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\n",
    "    {\n",
    "        key_1: {\n",
    "            key_2: {k: len(v) for k, v in value_2.items()}\n",
    "            for key_2, value_2 in value_1.items()\n",
    "        }\n",
    "        for key_1, value_1 in balanced_dataset.items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
